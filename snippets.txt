python CNNQAClassifier.py DecompCompCNN GoogleNews-vectors-negative300.bin stopwords.txt all_export.csv

def load_bag_of_words_based_neural_net_data(samples):
    qsdata = []
    ansdata = []
    labels = []
    for sample in samples:
        qsvec = get_bag_of_words_based_sentence_vec(sample.QsWords)
        ansvec=get_bag_of_words_based_sentence_vec(sample.AnsWords)
        qsdata.append(qsvec)
        ansdata.append(ansvec)
        labels.append(sample.Label)

    qsdata_nn = np.array(qsdata)
    ansdata_nn = np.array(ansdata)
    label_nn = np.array(labels)
    return qsdata_nn,ansdata_nn,label_nn

def get_bag_of_words_based_sentence_vec(words):
    vec = np.zeros(vec_dim, dtype='float32')
    word_count = 0
    for word in words:
        if stop_words.count(word) > 0:
            continue
        if word_vecs.has_key(word):
            vec += word_vecs[word]
        else:
            vec += word_vecs["<unk>"]
        word_count += 1
    #vec *= 100
    #vec /= word_count
    return vec

def run_neural_model(train_qsdata, train_ansdata, train_label, dev_qsdata, dev_ansdata, dev_ref_lines, test_qsdata, test_ansdata, test_ref_lines):

    qs_input = Input(shape=(vec_dim,), dtype='float32', name='qs_input')
    ans_input = Input(shape=(vec_dim,), dtype='float32', name='ans_input')
    qtm = Dense(output_dim=vec_dim, input_dim=vec_dim, activation='linear')(qs_input)
    merged = merge([qtm, ans_input], mode='dot', dot_axes=(1, 1))
    labels = Activation('sigmoid', name='labels')(merged)
    bow_model = Model(input=[qs_input, ans_input], output=[labels])

    bow_model.compile(loss={'labels': 'binary_crossentropy'}, optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),
                  metrics=['accuracy'])
    #SGD(lr=0.001, momentum=0.9, nesterov=True)

    batch_size = 100
    epoch = 50
    best_MAP = -10.0
    best_bow_model_file = os.path.join(data_folder, "best_bow_model.h5")

    for epoch_count in range(0, epoch):

        bow_model.fit({'qs_input': train_qsdata, 'ans_input': train_ansdata}, {'labels': train_label}, nb_epoch=1,
                  batch_size=batch_size, verbose=2)
        dev_probs = bow_model.predict([dev_qsdata, dev_ansdata], batch_size=batch_size)
        MAP, MRR = cal_score(dev_ref_lines, dev_probs)
        if MAP > best_MAP:
            best_MAP = MAP
            bow_model.save(best_bow_model_file)

    best_bow_model = load_model(best_bow_model_file)

    test_probs = best_bow_model.predict([test_qsdata, test_ansdata], batch_size=batch_size)

    MAP, MRR = cal_score(test_ref_lines, test_probs)
    return MAP, MRR

def run_bag_of_words_model(train_samples, dev_samples, test_samples, dev_ref_lines, test_ref_lines):

    train_qsdata, train_ansdata, train_label = load_bag_of_words_based_neural_net_data(train_samples)
    dev_qsdata, dev_ansdata, dev_label = load_bag_of_words_based_neural_net_data(dev_samples)
    test_qsdata, test_ansdata, test_label = load_bag_of_words_based_neural_net_data(test_samples)
    MAP, MRR = run_neural_model(train_qsdata, train_ansdata, train_label, dev_qsdata, dev_ansdata, dev_ref_lines, test_qsdata, test_ansdata, test_ref_lines)
    return MAP, MRR

def get_cnn_data(samples, max_qs_l, max_ans_l):
    qsdata = np.zeros(shape=(len(samples), max_qs_l, vec_dim), dtype="float32")
    ansdata = np.zeros(shape=(len(samples), max_ans_l, vec_dim), dtype="float32")
    labeldata = np.zeros(len(samples), dtype="int32")
    sent_count = 0
    for sample in samples:
        word_count = 0
        for word in sample.QsWords:
            if (word_vecs.has_key(word)):
                qsdata[sent_count][word_count] = word_vecs[word]
            else:
                qsdata[sent_count][word_count] = word_vecs["<unk>"]
            word_count += 1
        word_count = 0
        for word in sample.AnsWords:
            if (word_vecs.has_key(word)):
                ansdata[sent_count][word_count] = word_vecs[word]
            else:
                ansdata[sent_count][word_count] = word_vecs["<unk>"]
            word_count += 1
            if word_count==40:
                break
        labeldata[sent_count] = sample.Label
        sent_count += 1
    return qsdata,ansdata,labeldata



def train_cnn(ngram, data_folder, max_qs_l, max_ans_l,
              train_qsdata, train_ansdata, train_labeldata,
              dev_qsdata, dev_ansdata,
              test_qsdata, test_ansdata,
              dev_ref_lines, test_ref_lines):
    Reduce = Lambda(lambda x: x[:, 0, :], output_shape=lambda shape: (shape[0], shape[-1]))
    qs_input = Input(shape=(max_qs_l, vec_dim,), dtype='float32', name='qs_input')
    qsconvmodel = Convolution1D(nb_filter=sent_vec_dim, filter_length=ngram, activation="tanh", border_mode='valid')(
        qs_input)
    qsconvmodel = AveragePooling1D(pool_length=max_qs_l - ngram + 1)(qsconvmodel)
    qsconvmodel = Reduce(qsconvmodel)

    qtm = Dense(output_dim=sent_vec_dim, activation='linear')(qsconvmodel)

    ans_input = Input(shape=(max_ans_l, vec_dim,), dtype='float32', name='ans_input')
    ansconvmodel = Convolution1D(nb_filter=sent_vec_dim, filter_length=ngram, activation="tanh", border_mode='valid')(
        ans_input)
    ansconvmodel = AveragePooling1D(pool_length=max_ans_l - ngram + 1)(ansconvmodel)
    ansconvmodel = Reduce(ansconvmodel)

    merged = merge([qtm, ansconvmodel], mode='dot', dot_axes=(1, 1))
    labels = Activation('sigmoid', name='labels')(merged)
    cnn_model = Model(input=[qs_input, ans_input], output=[labels])

    cnn_model.compile(loss={'labels': 'binary_crossentropy'},
                  optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), metrics=['accuracy'])
    # Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    # Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)
    # SGD(lr=0.01, momentum=0.9, nesterov=True)


    batch_size = 10
    epoch = 20
    best_MAP=-10.0
    best_cnn_model_file=os.path.join(data_folder,"best_cnn_model.h5")
    train_probs_epochs=[]
    dev_probs_epochs=[]
    test_probs_epochs=[]
    for epoch_count in range(0,epoch):

        cnn_model.fit({'qs_input': train_qsdata, 'ans_input': train_ansdata}, {'labels': train_labeldata}, nb_epoch=1,
              batch_size=batch_size,verbose=2)
        train_probs=cnn_model.predict([train_qsdata, train_ansdata], batch_size=batch_size)
        train_probs_epochs.append(train_probs)
        dev_probs=cnn_model.predict([dev_qsdata,dev_ansdata],batch_size=batch_size)
        dev_probs_epochs.append(dev_probs)
        test_probs = cnn_model.predict([test_qsdata, test_ansdata], batch_size=batch_size)
        test_probs_epochs.append(test_probs)
        MAP, MRR=cal_score(dev_ref_lines,dev_probs)
        if MAP > best_MAP :
            best_MAP=MAP
            cnn_model.save(best_cnn_model_file)

    best_cnn_model=load_model(best_cnn_model_file)

    train_probs = best_cnn_model.predict([train_qsdata, train_ansdata], batch_size=batch_size)
    dev_probs=best_cnn_model.predict([dev_qsdata, dev_ansdata], batch_size=batch_size)
    test_probs = best_cnn_model.predict([test_qsdata, test_ansdata], batch_size=batch_size)

    MAP, MRR = cal_score(test_ref_lines, test_probs)

    return MAP, MRR, train_probs_epochs, dev_probs_epochs, test_probs_epochs, train_probs, dev_probs, test_probs

def train_lr_using_dense_layer(reg_train_data_np, reg_dev_data_np, reg_test_data_np, train_labeldata, dev_ref_lines, test_ref_lines):
    reg_feature_dim = len(reg_train_data_np[0])
    reg_input = Input(shape=(reg_feature_dim,), dtype='float32', name='reg_input')
    reg_layer = Dense(output_dim=1)(reg_input)
    reg_output = Activation('sigmoid', name='reg_output')(reg_layer)
    reg_model = Model(input=[reg_input], output=[reg_output])
    reg_model.compile(loss={'reg_output': 'binary_crossentropy'},
                      optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),
                      metrics=['accuracy'])
    batch_size = 1
    epoch=20
    best_MAP = -10.0
    best_reg_model_file = os.path.join(data_folder, "best_lr_dense_model.h5")
    for epoch_count in range(0, epoch):
        reg_model.fit({'reg_input': reg_train_data_np}, {'reg_output': train_labeldata}, nb_epoch=1,
                      batch_size=batch_size,verbose=2)

        dev_probs = reg_model.predict([reg_dev_data_np], batch_size=batch_size)
        MAP, MRR=cal_score(dev_ref_lines,dev_probs)
        if MAP > best_MAP:
            best_MAP=MAP
            reg_model.save(best_reg_model_file)
    best_reg_model=load_model(best_reg_model_file)
    test_probs=best_reg_model.predict([reg_test_data_np], batch_size=batch_size)

    MAP, MRR = cal_score(test_ref_lines, test_probs)
    return MAP, MRR

def train_lr_using_sklearn(train_probs_epochs, dev_probs_epochs, test_probs_epochs,
                           train_samples, dev_samples, test_samples, train_labeldata,
                           dev_ref_lines, test_ref_lines):
    best_dev_MAP=-10.0
    best_test_MAP=0.0
    best_test_MRR=0.0
    for i in range(0,len(train_probs_epochs)):
        train_probs=train_probs_epochs[i]
        dev_probs=dev_probs_epochs[i]
        test_probs=test_probs_epochs[i]
        reg_train_data_np = get_lr_data(train_samples, train_probs)
        reg_dev_data_np = get_lr_data(dev_samples, dev_probs)
        reg_test_data_np = get_lr_data(test_samples, test_probs)
        clf = linear_model.LogisticRegression(C=0.01, solver='lbfgs')
        clf = clf.fit(reg_train_data_np, train_labeldata)
        lr_dev_preds = clf.predict_proba(reg_dev_data_np)
        dev_probs = []
        for lr_dev_pred in lr_dev_preds:
            dev_probs.append(lr_dev_pred[1])
        dev_MAP, dev_MRR = cal_score(dev_ref_lines, dev_probs)

        lr_test_preds = clf.predict_proba(reg_test_data_np)
        test_probs = []
        for lr_test_pred in lr_test_preds:
            test_probs.append(lr_test_pred[1])
        test_MAP, test_MRR = cal_score(test_ref_lines, test_probs)
        if dev_MAP > best_dev_MAP :
            best_dev_MAP=dev_MAP
            best_test_MAP=test_MAP
            best_test_MRR=test_MRR

    return best_test_MAP, best_test_MRR

def get_lr_data(samples, probs):
    reg_data = []
    data_index = 0
    for sample in samples:
        feat = cali_feature_extractor(sample, probs[data_index])
        reg_data.append(feat)
        data_index += 1

    reg_data_np = np.array(reg_data)
    return reg_data_np

def run_bigram_model(ngram, train_samples, dev_samples, dev_ref_lines, test_samples, test_ref_lines):

    max_qs_l, max_ans_l=get_max_len(train_samples,dev_samples,test_samples)
    if max_ans_l > ans_len_cut_off:
        max_ans_l = ans_len_cut_off
    train_qsdata, train_ansdata, train_labeldata = get_cnn_data(train_samples, max_qs_l, max_ans_l)
    dev_qsdata, dev_ansdata, dev_labeldata=get_cnn_data(dev_samples, max_qs_l, max_ans_l)
    test_qsdata, test_ansdata, test_labeldata = get_cnn_data(test_samples, max_qs_l, max_ans_l)

    CNN_MAP, CNN_MRR, train_probs_epochs, dev_probs_epochs, test_probs_epochs, train_probs, dev_probs, test_probs=train_cnn(ngram, data_folder, max_qs_l, max_ans_l,
                                      train_qsdata, train_ansdata, train_labeldata,
                                      dev_qsdata, dev_ansdata,
                                      test_qsdata, test_ansdata,
                                      dev_ref_lines, test_ref_lines)


    reg_train_data_np=get_lr_data(train_samples,train_probs)
    reg_dev_data_np=get_lr_data(dev_samples,dev_probs)
    reg_test_data_np = get_lr_data(test_samples, test_probs)

    LR_Dense_MAP, LR_Dense_MRR = train_lr_using_dense_layer(reg_train_data_np, reg_dev_data_np, reg_test_data_np, train_labeldata, dev_ref_lines, test_ref_lines)

    #LR_Sklearn_MAP, LR_Sklearn_MRR= train_lr_using_sklearn(train_probs_epochs, dev_probs_epochs, test_probs_epochs, train_samples, dev_samples, test_samples, train_labeldata,
    #                                                   dev_ref_lines, test_ref_lines)

    return CNN_MAP, CNN_MRR, LR_Dense_MAP, LR_Dense_MRR #, LR_Sklearn_MAP, LR_Sklearn_MRR

def cal_score(ref_lines, probs):
    line_count = 0
    pred_lines = defaultdict(list)
    for ref_line in ref_lines:
        ref_line = ref_line.replace('\n', '')
        parts = ref_line.strip().split()
        qid, aid, lbl = int(parts[0]), int(parts[2]), int(parts[3])
        pred_lines[qid].append((aid, lbl, probs[line_count]))
        line_count += 1
    MAP=Score.calc_mean_avg_prec(pred_lines)
    MRR=Score.calc_mean_reciprocal_rank(pred_lines)
    return MAP, MRR

def clean_str(string):
    """
    Tokenization/string cleaning
    """
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    string = re.sub(r"\s{2,}", " ", string)
    return string.strip().lower()

def build_idf(files):
    n = 0
    ques_list, qtype_list = [], []
    for fname in files:
        with open(fname, "rb") as f:
            for line in f:
                n += 1
                parts = line.strip().split("\t")
                question=parts[0].replace('\n','')
                ques_list.append(question)
                question = clean_str(question)
                words = set(question.split())
                for word in words:
                    if word in stop_words:
                        continue
                    idf[word] += 1
    for word in idf.keys():
        idf[word] = np.log(n / idf[word])
    for fname in files:
        with open(fname[:fname.rfind(".") + 1] + "qtype", "rb") as f:
            for line in f:
                parts = line.strip().split(":")
                qtype_list.append(parts[0])
    for q, qt in zip(ques_list, qtype_list):
        qtype_map[q] = qt
        if qt not in qtype_invmap:
            qtype_invmap[qt] = len(qtype_invmap)
    return idf


def count_feature_extractor(qtoks, atoks):
    qset, aset = set(qtoks), set(atoks)
    count, weighted_count = 0.0, 0.0
    for word in qset:
        if word not in stop_words and word in aset:
            count += 1.0
            weighted_count += idf[word]
    return [count, weighted_count]


def cali_feature_extractor(sample, sim_probs):
    qtoks=sample.QsWords
    atoks=sample.AnsWords
    #if len(atoks) > ans_len_cut_off:
    #    atoks=atoks[0:ans_len_cut_off-1]
    question=sample.Qs
    question=question.replace('\n','')
    feat = count_feature_extractor(qtoks, atoks)
    feat.append(len(qtoks))
    ans_len=len(atoks)
    if ans_len > ans_len_cut_off:
        ans_len=ans_len_cut_off
    feat.append(ans_len)
    count, idf_sum = 1.0, 0.0
    for word in qtoks:
        if word not in stop_words:
            count += 1.0
            idf_sum += idf[word]
    feat.append(idf_sum / count)
    count, idf_sum = 1.0, 0.0
    for word in atoks:
        if word not in stop_words:
            count += 1.0
            idf_sum += idf[word]
    feat.append(idf_sum / count)
    qtype_vec = np.zeros(len(qtype_invmap))
    qtype_vec[qtype_invmap[qtype_map[question]]] = 1.0
    feat += qtype_vec.tolist()
    #for i in range(0,5):
    feat.append(sim_probs)
    return feat
